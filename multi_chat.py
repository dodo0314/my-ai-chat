import streamlit as st
import os
import json
import uuid
import time
from datetime import datetime
from openai import OpenAI

# ==========================================
# [ì„¤ì •] API í‚¤ ë° ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
# ==========================================
API_KEY = st.secrets["MY_API_KEY"]
SAVE_FOLDER = "chat_multi_data"

MODEL_OPTIONS = {
    "DeepSeek V3.2": "deepseek/deepseek-v3.2",
    "Sonnet 4.5": "anthropic/claude-sonnet-4.5", 
    "Grok-4.1": "x-ai/grok-4.1-fast",
    "Gemini 2.0_Free": "google/gemini-2.0-flash-exp:free",
    "mimo": "xiaomi/mimo-v2-flash:free",
}

if not os.path.exists(SAVE_FOLDER):
    os.makedirs(SAVE_FOLDER)

client = OpenAI(base_url="https://openrouter.ai/api/v1", api_key=API_KEY)

# ==========================================
# [í•¨ìˆ˜] ë°ì´í„° ê´€ë¦¬
# ==========================================
def load_chat(filename):
    if not filename: return []
    filepath = os.path.join(SAVE_FOLDER, filename)
    if os.path.exists(filepath):
        try:
            with open(filepath, "r", encoding="utf-8") as f:
                return json.load(f)
        except json.JSONDecodeError:
            return []
    return []

def save_chat(filename, history):
    filepath = os.path.join(SAVE_FOLDER, filename)
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(history, f, ensure_ascii=False, indent=2)

def get_chat_files():
    if not os.path.exists(SAVE_FOLDER): return []
    files = [f for f in os.listdir(SAVE_FOLDER) if f.endswith(".json")]
    files.sort(key=lambda x: os.path.getmtime(os.path.join(SAVE_FOLDER, x)), reverse=True)
    return files

def build_context(turn_history, slot_index):
    messages = []
    messages.append({"role": "system", "content": f"ë‹¹ì‹ ì€ {slot_index+1}ë²ˆ í™”ë©´ì˜ AIì…ë‹ˆë‹¤."})
    for turn in turn_history:
        messages.append({"role": "user", "content": turn["user"]})
        responses = turn.get("responses", {})
        str_idx = str(slot_index)
        if str_idx in responses:
            messages.append({"role": "assistant", "content": responses[str_idx]["text"]})
    return messages

# ==========================================
# [UI] í™”ë©´ êµ¬ì„±
# ==========================================
st.set_page_config(page_title="ë©€í‹° ê·¸ë¦¬ë“œ ì±—ë´‡", layout="wide")

with st.sidebar:
    st.title("ğŸ›ï¸ ë©€í‹° ì»¨íŠ¸ë¡¤")
    
    # 1. ì´ˆê¸°í™” ë¡œì§
    files = get_chat_files()
    if not files:
        init_file = f"New_Chat_{uuid.uuid4().hex[:4]}.json"
        save_chat(init_file, [])
        st.session_state["multi_chat_file"] = init_file
        st.rerun()
    
    if "multi_chat_file" not in st.session_state:
        st.session_state["multi_chat_file"] = files[0]

    # 2. í™”ë©´ ì„¤ì • (íƒ­ ëª¨ë“œ ì¶”ê°€ë¨!)
    st.subheader("1. í™”ë©´ ì„¤ì •")
    num_screens = st.radio("í™”ë©´ ë¶„í•  ê°œìˆ˜", [1, 2, 3, 4], horizontal=True, index=0)
    
    # â­ [NEW] ëª¨ë°”ì¼ìš© íƒ­ ëª¨ë“œ ìŠ¤ìœ„ì¹˜
    use_tabs = st.toggle("ğŸ“± ëª¨ë°”ì¼ íƒ­ ëª¨ë“œ (ì„¸ë¡œí˜•)", value=False)
    
    st.divider()
    
    # 3. ëª¨ë¸ ë°°ì •
    st.subheader("2. ëª¨ë¸ ë°°ì •")
    selected_models = []
    model_names = list(MODEL_OPTIONS.keys())
    
    for i in range(num_screens):
        default_idx = i % len(model_names)
        model_name = st.selectbox(
            f"ğŸ“º í™”ë©´ {i+1} ëª¨ë¸", 
            model_names, 
            index=default_idx,
            key=f"model_select_{i}"
        )
        selected_models.append(MODEL_OPTIONS[model_name])

    st.divider()
    
    # 4. ì±„íŒ…ë°© ëª©ë¡
    st.subheader("3. ì±„íŒ…ë°© ëª©ë¡")
    if st.button("â• ìƒˆ ì±„íŒ… ë§Œë“¤ê¸°", use_container_width=True):
        new_filename = f"New_Chat_{uuid.uuid4().hex[:4]}.json"
        save_chat(new_filename, [])
        st.session_state["multi_chat_file"] = new_filename
        st.rerun()

    files = get_chat_files()
    if st.session_state["multi_chat_file"] not in files and files:
         st.session_state["multi_chat_file"] = files[0]

    if files:
        current_file = st.radio("ëŒ€í™” ì„ íƒ", files, index=files.index(st.session_state["multi_chat_file"]) if st.session_state["multi_chat_file"] in files else 0, label_visibility="collapsed")
        if current_file != st.session_state["multi_chat_file"]:
            st.session_state["multi_chat_file"] = current_file
            st.rerun()
            
        st.markdown("---")
        st.caption("ğŸ“ ì´ë¦„ ë³€ê²½")
        current_filename = st.session_state["multi_chat_file"]
        new_name_input = st.text_input("íŒŒì¼ëª… ìˆ˜ì •", value=current_filename.replace(".json", ""), label_visibility="collapsed")
        
        if st.button("ì´ë¦„ ë³€ê²½ ì ìš©"):
            old_path = os.path.join(SAVE_FOLDER, current_filename)
            new_path = os.path.join(SAVE_FOLDER, f"{new_name_input}.json")
            if os.path.exists(new_path) and new_path != old_path:
                st.error("ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì´ë¦„ì…ë‹ˆë‹¤.")
            else:
                os.rename(old_path, new_path)
                st.session_state["multi_chat_file"] = f"{new_name_input}.json"
                st.success("ë³€ê²½ ì™„ë£Œ!")
                time.sleep(0.5)
                st.rerun()

# ë©”ì¸ í™”ë©´
safe_filename = st.session_state.get("multi_chat_file", "ìƒˆ ì±„íŒ…")
st.title(f"ğŸ§© {safe_filename.replace('.json', '')}")

history = load_chat(st.session_state.get("multi_chat_file"))

# ==========================================
# [ê¸°ë¡ ë Œë”ë§] íƒ­ ëª¨ë“œ ì ìš©
# ==========================================
for turn in history:
    with st.chat_message("user"):
        st.markdown(turn["user"])
    
    # â­ ì—¬ê¸°ê°€ í•µì‹¬ ë³€ê²½ í¬ì¸íŠ¸ (1)
    if use_tabs:
        containers = st.tabs([f"í™”ë©´ {i+1}" for i in range(num_screens)])
    else:
        containers = st.columns(num_screens)

    for i in range(num_screens):
        with containers[i]:
            resp_data = turn.get("responses", {}).get(str(i))
            if resp_data:
                tokens = resp_data.get('usage', {}).get('total_tokens', 'N/A')
                st.caption(f"ğŸ¤– {resp_data.get('model_name', 'AI')} | ğŸª™ Tokens: {tokens}")
                
                if "Error" in resp_data['text']:
                    st.error(resp_data['text'])
                else:
                    st.info(resp_data['text'])

# ==========================================
# [ì…ë ¥ ë° ì‹¤í–‰] íƒ­ ëª¨ë“œ ì ìš©
# ==========================================
if prompt := st.chat_input("ì§ˆë¬¸í•˜ê¸°..."):
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # â­ ì—¬ê¸°ê°€ í•µì‹¬ ë³€ê²½ í¬ì¸íŠ¸ (2)
    if use_tabs:
        containers = st.tabs([f"í™”ë©´ {i+1}" for i in range(num_screens)])
    else:
        containers = st.columns(num_screens)

    current_turn_responses = {}
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    for i in range(num_screens):
        with containers[i]:
            model_id = selected_models[i]
            display_name = [k for k, v in MODEL_OPTIONS.items() if v == model_id][0]
            
            st.caption(f"ğŸƒ Running: {display_name}...")
            msg_placeholder = st.empty()
            full_text = ""
            usage_info = {}
            
            context = build_context(history, i)
            context.append({"role": "user", "content": prompt})
            
            try:
                response = client.chat.completions.create(
                    model=model_id,
                    messages=context,
                    stream=True,
                    stream_options={"include_usage": True}
                )
                
                for chunk in response:
                    if chunk.choices and chunk.choices[0].delta.content:
                        full_text += chunk.choices[0].delta.content
                        msg_placeholder.info(full_text + "â–Œ")
                    
                    if chunk.usage:
                        usage_info = {
                            "prompt_tokens": chunk.usage.prompt_tokens,
                            "completion_tokens": chunk.usage.completion_tokens,
                            "total_tokens": chunk.usage.total_tokens
                        }

                msg_placeholder.info(full_text)
                
                current_turn_responses[str(i)] = {
                    "timestamp": timestamp,
                    "model_name": display_name,
                    "model_id": model_id,
                    "text": full_text,
                    "usage": usage_info
                }
            except Exception as e:
                err_msg = f"Error: {e}"
                msg_placeholder.error(err_msg)
                current_turn_responses[str(i)] = {
                    "timestamp": timestamp,
                    "model_name": display_name,
                    "model_id": model_id,
                    "text": err_msg,
                    "usage": {"error": str(e)}
                }

    if st.session_state.get("multi_chat_file"):
        new_turn = {"user": prompt, "responses": current_turn_responses}
        history.append(new_turn)

        save_chat(st.session_state["multi_chat_file"], history)

